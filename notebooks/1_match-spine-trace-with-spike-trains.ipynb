{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1) Match spine trace with extracellular spike trains\n",
    "\n",
    "In this notebook, we show how to match the corresponding presynaptic cell for a given dendritic spine. More detailed explanation can be found in ...\n",
    "\n",
    "First, the spike trains of all recorded units are convolved with an expotential kernel. This expotnetial kernel is constructed with parameters that fit the calcium indicator's dynamics. We then downsample the convolved spike trains in order to match the sampling rate of calcium imaging for postsynaptic cells.\n",
    "\n",
    "Second, we extract the spine calcium trace from the imaging. We use robust regression to remove the contribution of dendritic activities from the spine calcium trace in order to obtain the synaptic responses.\n",
    "\n",
    "Network synchrony (bursts) is detected and excluded for the following correlation analysis. We apply Pearson correlation to the spine calcium trace with convolved spike trains from all recorded units, and rank the correlation R values. In next notebook, we would show whether the unit with the highest R value should be accepted as the corresponding presynaptic cell for the given spine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import roiextractors as re\n",
    "import matplotlib.pyplot as plt\n",
    "import spikeextractors as se\n",
    "import belextractors as be\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import quantities as pq\n",
    "import neo as neo\n",
    "from elephant.statistics import instantaneous_rate\n",
    "from elephant import kernels\n",
    "import scipy.signal as ss\n",
    "import scipy.stats as stats\n",
    "import scipy.ndimage as ndimg\n",
    "import sys\n",
    "import pynwb\n",
    "import statsmodels.api as sm\n",
    "import spikewidgets as sw\n",
    "\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from imaging_tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Retrieve and load data\n",
    "\n",
    "NWB files can be downloaded from DANDI Archive \"https://gui.dandiarchive.org/#/dandiset/000223\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nwb files\n",
    "\n",
    "nwb_file_path = \"...\" # location of downloaded nwb file\n",
    "sorting, imag, imag_times, mea_times, mea_duration, ds_idxs = load_nwb_file(nwb_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2. Convolve extracellular spiketrains \n",
    "\n",
    "After loading the spike trains, we convolve them with an exponential decaying kernel to simulate the calcium response in a spine when an action potential is received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Neo spiketrains for convolution\n",
    "spiketrains = convert_to_neo(sorting, duration=mea_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve spike train with exponential kernel that fits the indicator kinetics\n",
    "fig_krn, ax_krn = plt.subplots()\n",
    "\n",
    "time_array = np.linspace(-1, 4, num=100) * pq.s\n",
    "kernel = kernels.ExponentialKernel(sigma=0.5*pq.s)\n",
    "kernel_time = kernel(time_array)\n",
    "ax_krn.plot(time_array, kernel_time)\n",
    "ax_krn.set_title(\"Exponential Kernel with sigma=0.5 s\")\n",
    "ax_krn.set_xlabel(\"time (s)\")\n",
    "ax_krn.set_ylabel(\"kernel (1/s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convolve each spike train with the exponential kernel (this is done with the `instantanous_rate` function in `elephant` using the exponential kernel we previously computed). After convolution, we downsample the convolved signal by using the shutter indices (`ds_idxs`) and we apply the $\\frac{\\Delta F}{F}$ transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# convolve spiketrains with kernel and calculate delta F over F (for normalization and detrending)\n",
    "individual_frs = []\n",
    "n_downsamples = imag.get_num_frames()\n",
    "individual_frs_ds = []\n",
    "individual_frs_ds_dff = []\n",
    "ts = np.linspace(0, mea_duration, n_downsamples)\n",
    "\n",
    "# params for delta F over F with percentile filtering\n",
    "PPRCTL = 25 \n",
    "PLEN = 100\n",
    "\n",
    "for i in tqdm(range(len(spiketrains)), desc=\"Computing istantaneous firing rates\"):\n",
    "    spt = spiketrains[i]\n",
    "    ifr = instantaneous_rate(spt, sampling_period=spt.sampling_period, center_kernel=False, \n",
    "                             kernel=kernel).squeeze()\n",
    "   \n",
    "    ifr_ds = ifr[ds_idxs].magnitude.astype(float) # downsampling\n",
    "    \n",
    "    baseline = 1 # avoid dividing by 0\n",
    "    ifr_ds_dff, ifr_ds_bl = dff(ifr_ds + baseline, 'winperc2', PLEN, PPRCTL)\n",
    "    \n",
    "    ifr_ds_dff[np.isnan(ifr_ds_dff)] = 0\n",
    "    ifr_ds_dff[np.isinf(ifr_ds_dff)] = 0\n",
    "    \n",
    "    individual_frs.append(ifr)\n",
    "    individual_frs_ds.append(ifr_ds)\n",
    "    individual_frs_ds_dff.append(ifr_ds_dff) # the covolved downsampled (same length with imag data) and normalized MEA traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure of an example of convolved trace\n",
    "fig_conv, ax_conv = plt.subplots()\n",
    "\n",
    "spike_id = 76 # type in a random unit id to check the convolved trace\n",
    "ax_conv.plot(spiketrains[spike_id].times, np.zeros_like(spiketrains[spike_id].times), '|', color='k',lw=0.1,\n",
    "             label=\"spike train\")\n",
    "ax_conv.plot(ts, individual_frs_ds_dff[spike_id],alpha = 0.7, lw=1, label=\"convolved trace\")\n",
    "ax_conv.set_title(\"Example of convoluted traces\")\n",
    "ax_conv.set_xlabel(\"time (s)\")\n",
    "ax_conv.set_ylabel(\"convolved signal (a.u.)\")\n",
    "ax_conv.legend()\n",
    "ax_conv.set_xlim([100, 150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Ca$^{2+}$ traces from imaging\n",
    "\n",
    "Here we extract the spine Ca$^{2+}$ traces and their adjacent dendritic shaft traces from the imaging. We can manually select multiple potential activated spines by drawing the ROIs and extract imaging data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load imag and manually select spine and adjacent dendritic shaft \n",
    "# multiple rois can be drawn at once for different spines and their adjacent dendritic shafts\n",
    "# the drawing order has to be: spine1,adjacent shaft of spine1, spine2, adjacent shaft of spine2, ...\n",
    "# example data can be loaded from nwb file below\n",
    "avg_im = compute_avg_video(imag, stride=500)\n",
    "rois = select_rois(avg_im) # select spines and respective shafts, in sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_DATA = True # if load fluorescence traces from nwb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_DATA:\n",
    "    print(\"Loading pre-computed Ca2+ traces\")\n",
    "    with pynwb.NWBHDF5IO(nwb_file_path, \"r\") as io:\n",
    "        read_nwbfile = io.read()\n",
    "        rois_response = read_nwbfile.processing['ophys']['Fluorescence']['RoiResponseSeries'].data[:]\n",
    "        trace_spine = np.transpose(rois_response)[0]\n",
    "        trace_shaft = np.transpose(rois_response)[1]\n",
    "        dfspine, blspine = dff(trace_spine, 'winperc2', PLEN, PPRCTL) # same params for percentile filter as for spiketrains\n",
    "        dfshaft, blshaft = dff(trace_shaft, 'winperc2', PLEN, PPRCTL)\n",
    "\n",
    "        traces_dff_spine = dfspine    \n",
    "        traces_dff_shaft = dfshaft\n",
    "        traces_bl_spine = blspine   \n",
    "        traces_bl_shaft = blshaft\n",
    "\n",
    "else:\n",
    "    traces_raw_spine = []\n",
    "    traces_raw_shaft = []\n",
    "\n",
    "    traces_dff_spine = []\n",
    "    traces_bl_spine = []\n",
    "    traces_dff_shaft = []\n",
    "    traces_bl_shaft = []\n",
    "    for i, (spine, shaft) in enumerate(zip(rois[::2], rois[1::2])):\n",
    "        trace_spine = extract_roi_activity(imag, spine)\n",
    "        trace_shaft = extract_roi_activity(imag, shaft)\n",
    "        dfspine, blspine = dff(trace_spine, 'winperc2', PLEN, PPRCTL)\n",
    "        dfshaft, blshaft = dff(trace_shaft, 'winperc2', PLEN, PPRCTL)\n",
    "        traces_raw_spine.append(trace_spine)\n",
    "        traces_raw_shaft.append(trace_shaft)\n",
    "        traces_dff_spine.append(dfspine)\n",
    "        traces_dff_shaft.append(dfshaft)\n",
    "        traces_bl_spine.append(blspine)\n",
    "        traces_bl_shaft.append(blshaft)\n",
    "\n",
    "    traces_raw_spine = np.array(traces_raw_spine)    \n",
    "    traces_raw_shaft = np.array(traces_raw_shaft)\n",
    "    traces_dff_spine = np.array(traces_dff_spine)    \n",
    "    traces_dff_shaft = np.array(traces_dff_shaft)\n",
    "    traces_bl_spine = np.array(traces_bl_spine)    \n",
    "    traces_bl_shaft = np.array(traces_bl_shaft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_cal, ax_cal = plt.subplots()\n",
    "# plot spine and dendritic traces\n",
    "if LOAD_DATA:\n",
    "    dff_spine = traces_dff_spine.copy()\n",
    "    dff_shaft = traces_dff_shaft.copy()\n",
    "else:\n",
    "    dff_spine = traces_dff_spine[0].copy()\n",
    "    dff_shaft = traces_dff_shaft[0].copy()    \n",
    "    \n",
    "ax_cal.plot(ts, dff_spine, label=\"spine\", alpha = 1, lw=1)\n",
    "ax_cal.plot(ts, dff_shaft, label=\"dendritic shaft\", alpha=0.5, lw=1)\n",
    "ax_cal.set_title(\"Calcium traces\")\n",
    "ax_cal.set_xlabel(\"time (s)\")\n",
    "ax_cal.set_ylabel(\"df/f (a.u.)\")\n",
    "ax_cal.legend()\n",
    "ax_cal.set_xlim([100, 150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exclusion of network and local synchrony\n",
    "\n",
    "Since the synchrony between spike trains will affect the correlation results strongly, we need to first detect the synchrony at network level and exclude the synchronized events. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first indentify and trim overactive units, as a unit that is hyper-active and continuously spiking will add complexity to any burst detector. Therefore, we need to detect such units and remove them before the burst detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fiter hyperactive channels\n",
    "\n",
    "# calculate spike rate of units\n",
    "single_unit_rates = []\n",
    "for i in range(len(spiketrains)):\n",
    "    single_unit_rates.append(len(spiketrains[i].times) / mea_duration)\n",
    "single_unit_rates = np.array(single_unit_rates)\n",
    "\n",
    "# compare the cutoff spike rates of percentile and a fixed value\n",
    "cutoff_pc = 99.75 # default percentile\n",
    "cutoff_hz = 15 # default fixed value\n",
    "\n",
    "logBins = np.power(10,np.arange(-2,2.2,0.2))\n",
    "numChans = np.histogram(single_unit_rates, bins=logBins)[0]\n",
    "\n",
    "lowess = sm.nonparametric.lowess # smoothing data with lowess method\n",
    "y = lowess(numChans, np.convolve(logBins,[0.5,0.5])[1:-1])[:,1] \n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogx(np.convolve(logBins,[0.5,0.5])[1:-1],y)\n",
    "plt.vlines(np.percentile(single_unit_rates,cutoff_pc), 0, np.max(y), colors='green', linestyles='dashed', label=f'{cutoff_pc} p.c', data=None)\n",
    "plt.vlines(cutoff_hz, 0, np.max(y), colors='red', linestyles='dashed', label=f'{cutoff_hz}Hz', data=None)\n",
    "plt.xlabel('Spike rate (Hz)')\n",
    "plt.ylabel('Count of units')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide the method used for cutoff (percentile or fixed value?)\n",
    "PC = True\n",
    "Hz = False\n",
    "\n",
    "if PC:\n",
    "    high_fr_units_id = np.where(single_unit_rates >= np.percentile(single_unit_rates,cutoff_pc))[0]\n",
    "if Hz:\n",
    "    high_fr_units_id = np.where(single_unit_rates >= cutoff_hz)[0]\n",
    "\n",
    "# remove the over active units for burst detection\n",
    "spikes_all_units = []\n",
    "spikes_unit_id = []\n",
    "for i in range(len(spiketrains)):\n",
    "    if i not in high_fr_units_id:\n",
    "        spikes_all_units.append(spiketrains[i].times)\n",
    "        spikes_unit_id.append([i]*len(spiketrains[i].times))\n",
    "        \n",
    "spikes_all_units = np.concatenate(spikes_all_units)\n",
    "spikes_all_units_sort = np.sort(spikes_all_units)\n",
    "spikes_id = np.concatenate(spikes_unit_id)[np.argsort(spikes_all_units)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement the burst detector based on global firing rate, the default setting of percentile threshold is 97 and for baseline is 85 percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burst detector based on global firing rate\n",
    "threshold_pctl = 97 # default parameter\n",
    "baseline_pctl = 85 # default parameter\n",
    "\n",
    "\n",
    "bin_width_s = 10e-3\n",
    "bin_vector = np.arange(0,np.round(mea_duration)+bin_width_s,bin_width_s)\n",
    "time_vector = np.convolve(bin_vector, [0.5,0.5])[1:-1]\n",
    "\n",
    "spikes_per_bin = np.histogram(spikes_all_units_sort, bins=bin_vector)[0]\n",
    "spikes_per_bin_sm = lowess(spikes_per_bin,np.arange(len(spikes_per_bin)),frac=0.0002)[:,1]\n",
    "threshold_rate = np.percentile(spikes_per_bin,threshold_pctl)\n",
    "baseline_rate = np.percentile(spikes_per_bin, baseline_pctl)\n",
    "\n",
    "peaks = detect_peaks(spikes_per_bin,mph=threshold_rate,mpd=50e-3/bin_width_s)\n",
    "peak_prominences = ss.peak_prominences(spikes_per_bin,peaks)[0]\n",
    "peak_pass_threshold = np.where(peak_prominences > np.percentile(spikes_per_bin,threshold_pctl))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find start and end times for bursts\n",
    "peak_times_ind = peaks[peak_pass_threshold]\n",
    "NetBurst_count = len(peak_times_ind)\n",
    "NetBurst_start = np.zeros(NetBurst_count)\n",
    "NetBurst_end = np.zeros(NetBurst_count)\n",
    "NetBurst_spikeCount = np.zeros(NetBurst_count)\n",
    "NetBurst_unitCount = np.zeros(NetBurst_count)\n",
    "                               \n",
    "for burst_i in range(NetBurst_count):\n",
    "    ionset = peak_times_ind[burst_i] - np.where(np.flipud(spikes_per_bin_sm[0:peak_times_ind[burst_i]]) <= baseline_rate)[0][0] + 1\n",
    "    ioffset = peak_times_ind[burst_i] + np.where(spikes_per_bin_sm[peak_times_ind[burst_i]::] <= baseline_rate)[0][0] - 1\n",
    "    tonset = time_vector[ionset]\n",
    "    toffset = time_vector[ioffset]\n",
    "    imask = (spikes_all_units_sort >= tonset) & (spikes_all_units_sort <= toffset) + 0\n",
    "    NetBurst_start[burst_i] = tonset\n",
    "    NetBurst_end[burst_i] = toffset\n",
    "    NetBurst_spikeCount[burst_i] = np.count_nonzero(imask)\n",
    "    NetBurst_unitCount[burst_i] = len(np.unique(spikes_id[np.where(imask == 1)[0]]))\n",
    "\n",
    "# Trimming edge cases -- bursts whose edges are beyond the data duration\n",
    "if any(NetBurst_start == 0) == True:\n",
    "    ind = np.where(NetBurst_start == 0)[0]\n",
    "    NetBurst_start = np.delete(NetBurst_start,ind)\n",
    "    NetBurst_end = np.delete(NetBurst_end,ind)\n",
    "    NetBurst_spikeCount = np.delete(NetBurst_spikeCount,ind)\n",
    "    NetBurst_count = NetBurst_count - np.count_nonzero(ind)\n",
    "    NetBurst_unitCount = np.delete(NetBurst_unitCount, ind)\n",
    "    \n",
    "if any(NetBurst_end == 0) == True:\n",
    "    ind = np.where(NetBurst_end == 0)[0]\n",
    "    NetBurst_start = np.delete(NetBurst_start,ind)\n",
    "    NetBurst_end = np.delete(NetBurst_end,ind)\n",
    "    NetBurst_spikeCount = np.delete(NetBurst_spikeCount,ind)\n",
    "    NetBurst_count = NetBurst_count - np.count_nonzero(ind)\n",
    "    NetBurst_unitCount = np.delete(NetBurst_unitCount, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expunging bursts with zero spikes\n",
    "# This could occur as an artifact of using the smoothed vector to detect burst edges\n",
    "\n",
    "if any(NetBurst_spikeCount == 0) == True:\n",
    "    empty_mask = np.where(NetBurst_spikeCount == 0)[0]\n",
    "    NetBurst_start = np.delete(NetBurst_start,empty_mask)\n",
    "    NetBurst_end = np.delete(NetBurst_end,empty_mask)\n",
    "    NetBurst_spikeCount = np.delete(NetBurst_spikeCount,empty_mask)\n",
    "    NetBurst_count = NetBurst_count - len(empty_mask)\n",
    "    NetBurst_unitCount = np.delete(NetBurst_unitCount, empty_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second pass to merge overlapping bursts\n",
    "min_ibi_ms = 75 #minimum interburst interval in ms. If smaller intervals are found, the involved bursts will be merged.\n",
    "\n",
    "tmp_IBI = (NetBurst_start[1::] - NetBurst_end[0:-1])\n",
    "i_negt = np.where(tmp_IBI < min_ibi_ms*1e-3)[0]\n",
    "nOverlap = len(i_negt)\n",
    "NetBurst_end[i_negt] = NetBurst_end[i_negt+1]\n",
    "NetBurst_start[i_negt+1] = np.nan\n",
    "NetBurst_end[i_negt+1] = np.nan\n",
    "NetBurst_spikeCount[i_negt+1] = 0\n",
    "NetBurst_unitCount[i_negt+1] = 0\n",
    "\n",
    "for i in range(nOverlap):\n",
    "    tonset = NetBurst_start[i_negt[i]]\n",
    "    toffset = NetBurst_end[i_negt[i]]\n",
    "    imask1 = (spikes_all_units_sort >= tonset) & (spikes_all_units_sort <= toffset) + 0\n",
    "    NetBurst_spikeCount[i_negt[i]] = np.count_nonzero(imask1)\n",
    "    NetBurst_unitCount[i_negt[i]] = len(np.unique(spikes_id[np.where(imask1 == 1)[0]]))\n",
    "    \n",
    "\n",
    "NetBurst_start = NetBurst_start[~np.isnan(NetBurst_start)]\n",
    "NetBurst_end = NetBurst_end[~np.isnan(NetBurst_end)]\n",
    "NetBurst_spikeCount = NetBurst_spikeCount[np.nonzero(NetBurst_spikeCount)[0]]\n",
    "NetBurst_count = NetBurst_count - nOverlap\n",
    "NetBurst_unitCount = NetBurst_unitCount[np.nonzero(NetBurst_unitCount)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IBI and Burst Width\n",
    "NetBurst_IBI_s = np.concatenate((np.nan,NetBurst_start[1::]-NetBurst_end[0:-1]),axis = None)\n",
    "NetBurst_burstWidth_ms = (NetBurst_end - NetBurst_start) * 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optinal (to remove small bursts)\n",
    "# if any (NetBurst_burstWidth_ms < 50) == True:\n",
    "#     ind2 = np.where(NetBurst_burstWidth_ms < 50)[0]\n",
    "#     NetBurst_start = np.delete(NetBurst_start,ind2)\n",
    "#     NetBurst_end = np.delete(NetBurst_end,ind2)\n",
    "#     NetBurst_spikeCount = np.delete(NetBurst_spikeCount,ind2)\n",
    "#     NetBurst_count = NetBurst_count - len(ind2)\n",
    "#     NetBurst_unitCount = np.delete(NetBurst_unitCount, ind2)\n",
    "#     NetBurst_burstWidth_ms = np.delete(NetBurst_burstWidth_ms, ind2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute peak recruitment within burst -- how many units are included in each bursting period\n",
    "peakRecruitment_bin_width_s = 5e-3\n",
    "peakRecruitment_numUnit = np.zeros((NetBurst_count))\n",
    "\n",
    "for burst_i in range(NetBurst_count):\n",
    "    imask = (spikes_all_units_sort >= NetBurst_start[burst_i]) & (spikes_all_units_sort <= NetBurst_end[burst_i])+0\n",
    "    bin_vector = np.arange(NetBurst_start[burst_i],NetBurst_end[burst_i],peakRecruitment_bin_width_s)\n",
    "    recruitment = np.histogram(spikes_all_units_sort[np.nonzero(imask)[0]], bins = bin_vector)[0]\n",
    "    bin_id = find_bin_id(recruitment)\n",
    "    pkid = np.argmax(recruitment)\n",
    "    units_in_burst = spikes_id[np.where(imask==1)[0]]\n",
    "    peakRecruitment_numUnit[burst_i] = len(np.unique(units_in_burst[np.where(bin_id==pkid)[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices to exclude network bursts for convolved traces and calcium traces\n",
    "Burst_start_ex = NetBurst_start - 0.1 # extended times of bursting period to fit the calcium indicator dynamics\n",
    "Burst_end_ex = NetBurst_end + 0.5\n",
    "\n",
    "ex_burst = []\n",
    "bs = np.transpose(Burst_start_ex)\n",
    "be = np.transpose(Burst_end_ex)\n",
    "\n",
    "for i, (st,e) in enumerate(zip(bs,be)):\n",
    "       for n in range(len(ts)-1):\n",
    "            if ts[n] > st and ts[n] < e:\n",
    "                ex_burst.append(n)\n",
    "                \n",
    "allidx = np.arange(len(trace_spine))\n",
    "keep_network_idxs = np.where(~np.in1d(allidx, ex_burst))[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_burst, ax_burst = plt.subplots()\n",
    "_ = sw.plot_rasters(sorting, ax=ax_burst, trange=(50, 100))\n",
    "\n",
    "for s, e in zip(bs, be):\n",
    "    ax_burst.axvspan(s, e, color='y', alpha=0.5, lw=0)\n",
    "\n",
    "_ = ax_burst.set_yticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Demixing dendritic signal from spine response\n",
    "\n",
    "The raw spine calcium trace contains not only synaptic activations but also dendritic signals, which is strong interference to correlating a spine to the corresponding presynaptic cell. Here we implemented robust regression to isolate the spine-specific signal,in order to get a clearer correlation with the presynaptic spike train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Demixing using robust fit\n",
    "\n",
    "traces_spine_dm = []\n",
    "traces_shaft_contrib = []\n",
    "traces_slope = []\n",
    "\n",
    "if LOAD_DATA:\n",
    "    DM_REF = traces_dff_shaft.copy()\n",
    "    DM_TARGET = traces_dff_spine.copy()\n",
    "else:\n",
    "    DM_REF = traces_dff_shaft[0].copy()\n",
    "    DM_TARGET = traces_dff_spine[0].copy()    \n",
    "\n",
    "# here we use traces with network burst exclusion for demixing\n",
    "dm_spine, shaft_contrib, slope, offset = demix_spine_shaft(DM_REF, DM_TARGET,\n",
    "                                                           method='huber', \n",
    "                                                           plot_fit=True) \n",
    "\n",
    "traces_spine_dm.append(dm_spine)\n",
    "traces_shaft_contrib.append(shaft_contrib)\n",
    "traces_slope.append(slope)\n",
    "    \n",
    "traces_spine_dm = np.array(traces_spine_dm)\n",
    "traces_shaft_contrib = np.array(traces_shaft_contrib)\n",
    "traces_slope = np.array(traces_slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform correlation for spine response with recorded units\n",
    "\n",
    "corr = np.zeros(len(individual_frs_ds_dff))\n",
    "\n",
    "\n",
    "for i, ifr in enumerate(individual_frs_ds_dff):\n",
    "              \n",
    "    rval, pval = stats.pearsonr(ifr[keep_network_idxs], traces_spine_dm[0][keep_network_idxs]) \n",
    "    corr[i] = rval                                            \n",
    "       \n",
    "        \n",
    "best_idxs = np.argsort(corr[:])[::-1][0:5] # get indices of the first 5 best match\n",
    "\n",
    "print(best_idxs)\n",
    "print(corr[best_idxs]) # print correlation r-value from the first 5 best match\n",
    "\n",
    "best_unit = best_idxs[0]\n",
    "\n",
    "# bar plot of sorted correlation r-value between the spine response and all recorded MEA spiketrains (high --> low)\n",
    "plt.figure() \n",
    "plt.bar(x=np.arange(len(corr[:])), height=corr[np.argsort(corr[:])].ravel()[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the demixed spine response and convolved trace from the best match\n",
    "plt.figure()\n",
    "shaft_contribution_full = slope * DM_REF + offset\n",
    "demixed_spine_full = DM_TARGET - shaft_contribution_full\n",
    "unit = individual_frs_ds_dff[best_unit].copy()\n",
    "\n",
    "n = 0.15 # unit trace need a scaling factor because the linear summation of multiple spikes, adjustable\n",
    "plt.plot(ts,demixed_spine_full,label = 'demixed_spine')\n",
    "plt.plot(ts, unit*n, alpha = 0.5, label = 'convolved unit trace') \n",
    "plt.plot(spiketrains[best_unit].times, np.zeros_like(spiketrains[best_unit].times), '|', color='k',lw=0.1,\n",
    "             label=\"spike train\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Save data for next parts\n",
    "\n",
    "To continue with next part, save all variables needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('saved_data_P1.npz', spiketrains = spiketrains, best_match=best_unit, best_r=corr[best_unit], second_match=best_idxs[1],\n",
    "          individual_frs_ds_dff=individual_frs_ds_dff, keep=keep_network_idxs) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
